{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Tech Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)\n",
    "# conda install pytorch==1.13.0 torchvision==0.14.0 torchaudio==0.13.0 pytorch-cuda=11.7 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('QuanTech_R/R file/quant_pub.csv')\n",
    "df['qc_category1'] = df['qc_category'].str[:3]\n",
    "df['desc'] = df['abstract'].fillna(df['itemtitle'])\n",
    "# Pre-processing\n",
    "df['cleaned_docs'] = [s.replace('\\r', '').replace('\\n', '').replace('<p>', '').replace('</p>', '') for s in df['desc']]\n",
    "df['cleaned_docs'] = [re.sub(r'\\(C\\) 20.*', '', text) for text in df['cleaned_docs']]\n",
    "df = df[df['cleaned_docs']!='Editorial Board']\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)\n",
    "\n",
    "df = df[['pubid', 'pubyear', 'cleaned_docs', 'qc_category1']]\n",
    "df = df.drop_duplicates()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.qc_category1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 692/692 [00:15<00:00, 45.03it/s]\n",
      "2024-05-03 08:45:55,667 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n",
      "Batches: 100%|██████████| 1897/1897 [00:43<00:00, 43.75it/s] \n",
      "2024-05-03 08:47:51,186 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n",
      "Batches: 100%|██████████| 115/115 [00:02<00:00, 40.74it/s]\n",
      "2024-05-03 08:48:17,774 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n",
      "Batches: 100%|██████████| 2901/2901 [01:07<00:00, 42.91it/s] \n",
      "2024-05-03 08:51:17,086 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "qc_type = df.qc_category1.unique()\n",
    "\n",
    "for i in qc_type:\n",
    "    temp_df = df.loc[df.qc_category1 == i]\n",
    "    docs = temp_df['cleaned_docs'].to_list()\n",
    "    \n",
    "    from bertopic import BERTopic\n",
    "    from umap import UMAP\n",
    "    from hdbscan import HDBSCAN\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from bertopic import BERTopic\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "    from bertopic.vectorizers import ClassTfidfTransformer\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "    # Set Diveristy of Topics (0: no diversity, 1: max diversity)\n",
    "    from bertopic.representation import MaximalMarginalRelevance\n",
    "    # representation_model = MaximalMarginalRelevance(diversity=0.5)\n",
    "\n",
    "    # Set embedding model\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    # Pre-calculate embeddings\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = embedding_model.encode(docs, \n",
    "                                        show_progress_bar=True)\n",
    "\n",
    "    # Hyper parameter setting\n",
    "    n_gram_ranges = (1, 1)\n",
    "    min_topic_sizes = int(len(docs)*0.001)\n",
    "    nr_topics_options = 20\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "            n_gram_range=n_gram_ranges,\n",
    "            min_topic_size = min_topic_sizes, \n",
    "            nr_topics = nr_topics_options, \n",
    "            embedding_model=embedding_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "    #         representation_model=representation_model,\n",
    "            calculate_probabilities=True,\n",
    "            ctfidf_model = ctfidf_model,\n",
    "            umap_model = umap_model \n",
    "        )\n",
    "    \n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "    \n",
    "    freq = topic_model.get_topic_info()\n",
    "    freq.to_csv('QuanTech_Python/freq_'+ i +'_hur.csv', index=False)\n",
    "\n",
    "    topic_model_sum = topic_model.get_document_info(docs)\n",
    "    \n",
    "    topic_model_sum = pd.merge(temp_df[['pubid','cleaned_docs']], \n",
    "                            topic_model_sum.loc[:, ~topic_model_sum.columns.isin(['Name''Representation','Representative_Docs'])], \n",
    "                            left_on='cleaned_docs', right_on='Document')\n",
    "\n",
    "    topic_model_sum.to_csv(\"QuanTech_Python/\"+i+\"_sumtable_hur.csv\")\n",
    "    topic_model.save(\"QuanTech_Python/\"+i+\"bertopic_model_hur\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic Label Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/LLM/meta-llama/Meta-Llama-3-8B-Instruct\", use_auth_token=True) # meta-llama/Meta-Llama-3-70B-Instruct\n",
    "model = LlamaForCausalLM.from_pretrained(\"D:/LLM/meta-llama/Meta-Llama-3-8B-Instruct\", #meta-llama/Meta-Llama-3-70B-Instruct\n",
    "    torch_dtype=torch.float16)\n",
    "\n",
    "# model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "### Get list of files\n",
    "files = glob.glob('QuanTech_Python/freq_*_hur.csv')\n",
    "files = [file for file in files if '_gentopic' not in file]\n",
    "\n",
    "### Iterative statments for generating \"GenTopic\"\n",
    "for file_name in files:\n",
    "    print(file_name)\n",
    "\n",
    "    period_name = re.search(r'freq_(.*).csv', file_name).group(1)\n",
    "\n",
    "    ### Data load & preparation\n",
    "    dat = pd.read_csv(file_name)\n",
    "\n",
    "    dat = dat[dat.Topic != -1] # needed if want to remove outlier\n",
    "    dat = dat.reset_index()\n",
    "    dat['GenTopic'] = \"\" \n",
    "\n",
    "    prompt = \"\"\".\\n List of words above are the topic modelling result from quantum technology publications. Generate a label that summarizes them. \"\"\"\n",
    "    \n",
    "    for i in dat.index:\n",
    "        inputs = tokenizer(dat['Representation'][i]+prompt, return_tensors='pt')\n",
    "        inputs.to(device)\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length = 150, pad_token_id=tokenizer.eos_token_id)\n",
    "        dat['GenTopic'][i] = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \n",
    "        dat['GenTopic'][i] = dat['GenTopic'][i].replace(dat['Representation'][i], \"\")\n",
    "        dat['GenTopic'][i] = dat['GenTopic'][i].replace(prompt, \"\")\n",
    "            \n",
    "    dat.to_csv(\"QuanTech_Python/freq_\"+period_name+\"_gentopic_llama3_hur.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
