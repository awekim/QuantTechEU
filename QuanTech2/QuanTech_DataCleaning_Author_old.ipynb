{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c905e208",
   "metadata": {},
   "source": [
    "### Old version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "dir = 'D:/GD_awekimm/[YU]/[Project]/[Quantum]/Quantum_2nd/04_Analysis/QuanTech_R2/R file/'\n",
    "\n",
    "# 1. 데이터 로딩 (기존과 동일)\n",
    "df = pd.read_csv(dir+'quant_author_ed_eu_val.csv')\n",
    "inst = pd.read_csv(dir+'quant_inst_ed_eu_val_cleaned.csv')\n",
    "\n",
    "required_columns = [\n",
    "    'pubid', 'city', 'country', 'pubyear', 'author_id_te', \n",
    "    'full_name', 'matched', 'organization_cleaned', 'suborganization_cleaned'\n",
    "]\n",
    "df = df[required_columns]\n",
    "inst = inst[['pubid', 'organization_cleaned', 'suborganization_cleaned']]\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df['full_name'] = df['full_name'].str.replace(r'\\.$', '', regex=True)\n",
    "\n",
    "# 2. '정답' 그룹과 '처리 대상' 그룹 분리 (기존과 동일)\n",
    "df_matched = df[df['matched'] == 'matched'].copy()\n",
    "df_rest = df[df['matched'] == 'rest'].copy()\n",
    "\n",
    "# =================================================================\n",
    "# ⭐️ 3. (신규) 'rest' 데이터 소속 기관 추론 로직 ⭐️\n",
    "# =================================================================\n",
    "\n",
    "# 3-1. (준비) 추론에 필요한 정보 생성\n",
    "# 'matched' 데이터에서 기관-위치 정보 맵 생성\n",
    "inst_location_map = df_matched.groupby('organization_cleaned')[['city', 'country']].first().to_dict('index')\n",
    "# 'matched' 데이터에서 논문별(pubid) 소속 기관 정보 생성\n",
    "pub_org_map = df_matched.groupby('pubid')['organization_cleaned'].unique().apply(list).to_dict()\n",
    "\n",
    "# 3-2. (증거 수집) 각 'rest' 레코드에 대한 후보 기관 수집\n",
    "candidate_orgs = []\n",
    "for idx, row in df_rest.iterrows():\n",
    "    pubid = row['pubid']\n",
    "    city = row['city']\n",
    "    country = row['country']\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # [증거 1: 공동 저자] 같은 논문의 'matched' 저자 소속 기관을 후보로 추가\n",
    "    if pubid in pub_org_map:\n",
    "        candidates.extend(pub_org_map[pubid])\n",
    "        \n",
    "    # [증거 2: 지리 정보] 해당 논문의 모든 연관 기관 중, 위치가 일치하는 기관을 후보로 추가\n",
    "    # inst 테이블에는 한 pubid에 여러 기관이 있을 수 있음\n",
    "    possible_orgs = inst[inst['pubid'] == pubid]['organization_cleaned'].unique()\n",
    "    for org in possible_orgs:\n",
    "        if org in inst_location_map:\n",
    "            loc = inst_location_map[org]\n",
    "            if loc['city'] == city and loc['country'] == country:\n",
    "                candidates.append(org)\n",
    "\n",
    "    candidate_orgs.append(candidates)\n",
    "\n",
    "df_rest['candidate_orgs'] = candidate_orgs\n",
    "\n",
    "# --- 하이퍼파라미터 설정 ---\n",
    "# '핵심 저자'로 판단할 최소 논문 수\n",
    "FREQUENCY_THRESHOLD = 5\n",
    "# '핵심 저자'의 기관을 결정하기 위한 최소 신뢰도(증거 일치율)\n",
    "HIGH_CONFIDENCE_THRESHOLD = 0.8  # 80%\n",
    "\n",
    "# 3-3-1. 저자별 논문 수를 계산하여 '핵심 저자' 그룹을 정의\n",
    "author_counts = df['full_name'].value_counts()\n",
    "frequent_authors = set(author_counts[author_counts >= FREQUENCY_THRESHOLD].index)\n",
    "\n",
    "# 3-3-2. 저자 프로필 생성 (기존과 동일)\n",
    "author_profiles = {}\n",
    "# 'matched' 데이터로 프로필 초기화\n",
    "for _, row in df_matched.iterrows():\n",
    "    author = row['full_name']\n",
    "    org = row['organization_cleaned']\n",
    "    if author not in author_profiles:\n",
    "        author_profiles[author] = []\n",
    "    author_profiles[author].append(org)\n",
    "\n",
    "# 'rest' 데이터의 후보 기관들을 프로필에 추가\n",
    "for _, row in df_rest.iterrows():\n",
    "    author = row['full_name']\n",
    "    if author not in author_profiles:\n",
    "        author_profiles[author] = []\n",
    "    author_profiles[author].extend(row['candidate_orgs'])\n",
    "\n",
    "# 3-3-3. 저자별로 중요도에 따라 다르게 'inferred_org' 결정\n",
    "inferred_org_map = {}\n",
    "for author, org_list in author_profiles.items():\n",
    "    if not org_list:\n",
    "        continue\n",
    "\n",
    "    counts = Counter(org_list)\n",
    "    most_common_org, top_count = counts.most_common(1)[0]\n",
    "    \n",
    "    # [Safe Mode] 저자가 '핵심 저자' 그룹에 속하는 경우\n",
    "    if author in frequent_authors:\n",
    "        total_evidence = len(org_list)\n",
    "        confidence = top_count / total_evidence\n",
    "        \n",
    "        # 신뢰도가 설정된 임계값을 넘을 때만 기관을 할당\n",
    "        if confidence >= HIGH_CONFIDENCE_THRESHOLD:\n",
    "            inferred_org_map[author] = most_common_org\n",
    "        # 넘지 못하면 실수를 피하기 위해 할당하지 않음 (결과적으로 NaN)\n",
    "            \n",
    "    # [Standard Mode] 저자가 '일반 저자' 그룹에 속하는 경우\n",
    "    else:\n",
    "        # 기존 방식대로 가장 빈도가 높은 기관을 할당\n",
    "        inferred_org_map[author] = most_common_org\n",
    "\n",
    "# 3-4-1. 추론된 주 기관명을 먼저 할당합니다.\n",
    "df_rest['organization_cleaned'] = df_rest['full_name'].map(inferred_org_map)\n",
    "\n",
    "# 3-4-2. 기관명에 맞는 city, country를 매핑하기 위한 map을 준비합니다.\n",
    "# inst_location_map에서 city와 country를 위한 별도의 딕셔너리를 생성합니다.\n",
    "city_map = {org: loc['city'] for org, loc in inst_location_map.items()}\n",
    "country_map = {org: loc['country'] for org, loc in inst_location_map.items()}\n",
    "\n",
    "# 3-4-3. city와 country 정보를 업데이트합니다.\n",
    "# 추론된 organization_cleaned를 기준으로 새로운 위치 정보를 매핑합니다.\n",
    "# 만약 매핑되는 정보가 없다면(NaN), 기존의 위치 정보를 그대로 사용(fillna)합니다.\n",
    "df_rest['city'] = df_rest['organization_cleaned'].map(city_map).fillna(df_rest['city'])\n",
    "df_rest['country'] = df_rest['organization_cleaned'].map(country_map).fillna(df_rest['country'])\n",
    "\n",
    "# 3-4-4. suborganization은 여전히 알 수 없으므로 NaN 처리합니다.\n",
    "df_rest['suborganization_cleaned'] = np.nan \n",
    "\n",
    "# 추론에 사용된 임시 컬럼 삭제\n",
    "df_rest = df_rest.drop(columns=['candidate_orgs'])\n",
    "\n",
    "\n",
    "# 4. 보호된 그룹과 처리된 그룹 다시 결합 (기존과 유사)\n",
    "# df_rest에서 기관 정보가 없는(추론 실패한) 경우는 제외하거나 포함할 수 있음. 여기서는 포함.\n",
    "df_final = pd.concat([df_matched, df_rest], ignore_index=True)\n",
    "\n",
    "\n",
    "# 5. ID 통일 로직 실행 (기존과 동일)\n",
    "# ... (이하 ID 통일 및 저장 코드는 모두 동일합니다) ...\n",
    "\n",
    "# Step 1. 그룹별 canonical ID 찾기\n",
    "min_ids = df_final.groupby(['full_name', 'organization_cleaned'])['author_id_te'].min()\n",
    "matched_ids = df_final[df_final['matched'] == 'matched'].groupby(['full_name', 'organization_cleaned'])['author_id_te'].first()\n",
    "canonical_s = matched_ids.combine_first(min_ids)\n",
    "canonical_ids = canonical_s.reset_index(name='canonical_id')\n",
    "canonical_ids['canonical_id'] = canonical_ids['canonical_id'].astype(df_final['author_id_te'].dtype)\n",
    "\n",
    "# Step 2. df에 canonical ID 병합\n",
    "df_final = df_final.merge(canonical_ids, on=['full_name','organization_cleaned'], how='left')\n",
    "\n",
    "# Step 3. 'matched' ID를 최우선으로 하여 ID 전파\n",
    "final_id_from_matched = df_final[df_final['matched'] == 'matched'].groupby('full_name')['canonical_id'].first()\n",
    "default_final_id = df_final.groupby('full_name')['canonical_id'].min()\n",
    "final_id_map = final_id_from_matched.combine_first(default_final_id)\n",
    "df_final['author_id_te_cleaned'] = df_final['full_name'].map(final_id_map)\n",
    "\n",
    "# Step 4. cleaned 여부 표시\n",
    "df_final['cleaned_or_not'] = np.where(\n",
    "    df_final['author_id_te'] == df_final['author_id_te_cleaned'],\n",
    "    'original',\n",
    "    'cleaned'\n",
    ")\n",
    "\n",
    "# 6. 최종 정리 및 저장 (기존과 동일)\n",
    "df_final = df_final.drop(columns=['canonical_id'])\n",
    "df_final = df_final.drop_duplicates(subset=['author_id_te_cleaned', 'pubid', 'organization_cleaned'])\n",
    "\n",
    "df_final.to_parquet(dir + 'quant_author_ed_eu_val_cleaned_inferred.parquet')\n",
    "\n",
    "print(\"데이터 정제 및 저장이 완료되었습니다.\")\n",
    "print(\"추론 후 데이터 샘플:\")\n",
    "print(df_final[df_final['matched'] == 'rest'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab2d22",
   "metadata": {},
   "source": [
    "### Old version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dir = 'D:/GD_awekimm/[YU]/[Project]/[Quantum]/Quantum_2nd/04_Analysis/QuanTech_R2/R file/'\n",
    "\n",
    "# 1. 데이터 로딩\n",
    "df = pd.read_csv(dir+'quant_author_ed_eu_val.csv')\n",
    "inst = pd.read_csv(dir+'quant_inst_ed_eu_val_cleaned.csv')\n",
    "\n",
    "# ⭐️ (핵심) 원본 파일에 '정답' 기관 정보가 반드시 포함되어 있어야 합니다.\n",
    "# 만약 실제 컬럼 이름이 다르다면 이 부분을 꼭 수정해주세요.\n",
    "required_columns = [\n",
    "    'pubid', 'city', 'country', 'pubyear', 'author_id_te', \n",
    "    'full_name', 'matched', 'organization_cleaned', 'suborganization_cleaned'\n",
    "]\n",
    "df = df[required_columns]\n",
    "inst = inst[['pubid', 'organization_cleaned', 'suborganization_cleaned']]\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df['full_name'] = df['full_name'].str.replace(r'\\.$', '', regex=True)\n",
    "\n",
    "\n",
    "# 2. '정답' 그룹과 '처리 대상' 그룹 분리\n",
    "df_matched = df[df['matched'] == 'matched'].copy()\n",
    "df_rest = df[df['matched'] == 'rest'].copy()\n",
    "\n",
    "\n",
    "# 3. 각 그룹 처리\n",
    "# 3-1. (보호) 'matched' 그룹은 '정답'이므로 아무것도 하지 않고 그대로 사용합니다.\n",
    "#       inst 파일과 병합하지 않으므로 원본 정보가 100% 보존됩니다.\n",
    "\n",
    "# 3-2. (처리) 'rest' 그룹은 기관 정보가 불확실하므로 inst 파일과 병합합니다.\n",
    "#       병합 전, 기존의 (불확실한) 기관 컬럼을 삭제하여 충돌을 방지합니다.\n",
    "df_rest = df_rest.drop(columns=['organization_cleaned', 'suborganization_cleaned'])\n",
    "df_rest = pd.merge(df_rest, inst, on='pubid', how='left')\n",
    "del inst\n",
    "\n",
    "\n",
    "# 4. 보호된 그룹과 처리된 그룹 다시 결합\n",
    "df_final = pd.concat([df_matched, df_rest], ignore_index=True)\n",
    "\n",
    "\n",
    "# 5. ID 통일 로직 실행 (이제 안전하게 조합된 데이터를 대상으로 실행)\n",
    "# Step 1. 그룹별 canonical ID 찾기\n",
    "min_ids = df_final.groupby(['full_name', 'organization_cleaned'])['author_id_te'].min()\n",
    "matched_ids = df_final[df_final['matched'] == 'matched'].groupby(['full_name', 'organization_cleaned'])['author_id_te'].first()\n",
    "canonical_s = matched_ids.combine_first(min_ids)\n",
    "canonical_ids = canonical_s.reset_index(name='canonical_id')\n",
    "canonical_ids['canonical_id'] = canonical_ids['canonical_id'].astype(df_final['author_id_te'].dtype)\n",
    "\n",
    "# Step 2. df에 canonical ID 병합\n",
    "df_final = df_final.merge(canonical_ids, on=['full_name','organization_cleaned'], how='left')\n",
    "\n",
    "# Step 3. 'matched' ID를 최우선으로 하여 ID 전파\n",
    "final_id_from_matched = df_final[df_final['matched'] == 'matched'].groupby('full_name')['canonical_id'].first()\n",
    "default_final_id = df_final.groupby('full_name')['canonical_id'].min()\n",
    "final_id_map = final_id_from_matched.combine_first(default_final_id)\n",
    "df_final['author_id_te_cleaned'] = df_final['full_name'].map(final_id_map)\n",
    "\n",
    "# Step 4. cleaned 여부 표시\n",
    "df_final['cleaned_or_not'] = np.where(\n",
    "    df_final['author_id_te'] == df_final['author_id_te_cleaned'],\n",
    "    'original',\n",
    "    'cleaned'\n",
    ")\n",
    "\n",
    "# 6. 최종 정리 및 저장\n",
    "df_final = df_final.drop(columns=['canonical_id'])\n",
    "# 중복 제거 기준을 더 명확히 하여 데이터 안정성 확보\n",
    "df_final = df_final.drop_duplicates(subset=['author_id_te_cleaned', 'pubid', 'organization_cleaned'])\n",
    "\n",
    "# 경로에 특수문자('[', ']')가 없는지 다시 한번 확인해주세요.\n",
    "df_final.to_parquet(dir + 'quant_author_ed_eu_val_cleaned.parquet')\n",
    "\n",
    "print(\"데이터 정제 및 저장이 완료되었습니다.\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9a4c1",
   "metadata": {},
   "source": [
    "### Old version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe37188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
