{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb8feaa",
   "metadata": {},
   "source": [
    "# **양자과학기술 저자명 통일**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814aee5",
   "metadata": {},
   "source": [
    "### Hybrid version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Set directory path\n",
    "dir = 'D:/GD_awekimm/[YU]/[Project]/[Quantum]/Quantum_2nd/04_Analysis/QuanTech_R2/R file/'\n",
    "\n",
    "# =================================================================\n",
    "# Step 1: Data Loading and ID Unification\n",
    "# =================================================================\n",
    "print(\"[Step 1] ...\")\n",
    "\n",
    "# Dta loading & preprocessing\n",
    "df = pd.read_csv(dir+'quant_author_ed_eu_val.csv')\n",
    "inst = pd.read_csv(dir+'quant_inst_ed_eu_val_cleaned.csv')\n",
    "\n",
    "# Required columns\n",
    "required_columns = [\n",
    "    'pubid', 'city', 'country', 'pubyear', 'author_id_te',\n",
    "    'full_name', 'matched', 'organization_cleaned'\n",
    "]\n",
    "df = df[required_columns].drop_duplicates()\n",
    "df['full_name'] = df['full_name'].str.replace(r'\\.$', '', regex=True)\n",
    "inst = inst[['pubid', 'organization_cleaned']]\n",
    "\n",
    "# Prepare data for ID unification\n",
    "df_matched = df[df['matched'] == 'matched'].copy()\n",
    "df_rest = df[df['matched'] == 'rest'].copy()\n",
    "df_rest_for_id = pd.merge(\n",
    "    df_rest.drop(columns=['organization_cleaned']),\n",
    "    inst, on='pubid', how='left'\n",
    ")\n",
    "df_for_id_unification = pd.concat([df_matched, df_rest_for_id], ignore_index=True)\n",
    "\n",
    "# Unify author IDs within same (name + organization) groups\n",
    "min_ids = df_for_id_unification.groupby(['full_name', 'organization_cleaned'])['author_id_te'].min()\n",
    "matched_ids = df_for_id_unification[df_for_id_unification['matched'] == 'matched'].groupby(['full_name', 'organization_cleaned'])['author_id_te'].first()\n",
    "canonical_s = matched_ids.combine_first(min_ids)\n",
    "canonical_ids = canonical_s.reset_index(name='canonical_id')\n",
    "df_for_id_unification = df_for_id_unification.merge(canonical_ids, on=['full_name', 'organization_cleaned'], how='left')\n",
    "\n",
    "# Generate final author IDs based on matched records\n",
    "final_id_from_matched = df_for_id_unification[df_for_id_unification['matched'] == 'matched'].groupby('full_name')['canonical_id'].first()\n",
    "default_final_id = df_for_id_unification.groupby('full_name')['canonical_id'].min()\n",
    "final_id_map = final_id_from_matched.combine_first(default_final_id)\n",
    "df_for_id_unification['author_id_te_cleaned'] = df_for_id_unification['full_name'].map(final_id_map)\n",
    "\n",
    "# Add unified IDs back to the 'rest' dataset\n",
    "df_rest = pd.merge(df_rest, df_for_id_unification[['pubid', 'author_id_te', 'author_id_te_cleaned']].drop_duplicates(), on=['pubid', 'author_id_te'])\n",
    "print(\"ID unification complete.\")\n",
    "\n",
    "# =================================================================\n",
    "# Step 2. Rule-Based Precise Matching (PASS 1)\n",
    "# =================================================================\n",
    "print(\"\\n[Step 2] PASS 1: Rule-based matching...\")\n",
    "\n",
    "author_history = df_for_id_unification.groupby('author_id_te_cleaned')['organization_cleaned'].unique().apply(list)\n",
    "pub_orgs = df_for_id_unification.groupby('pubid')['organization_cleaned'].unique().apply(list)\n",
    "forbidden_orgs_map = df_for_id_unification[df_for_id_unification['matched'] == 'matched'].groupby('pubid')['organization_cleaned'].first()\n",
    "\n",
    "def find_likely_org_with_rule(row, author_history_map, pub_orgs_map, forbidden_map):\n",
    "    author_id, pub_id = row['author_id_te_cleaned'], row['pubid']\n",
    "    history_orgs = author_history_map.get(author_id, [])\n",
    "    candidate_orgs = pub_orgs_map.get(pub_id, []).copy()\n",
    "    forbidden_org = forbidden_map.get(pub_id)\n",
    "    if forbidden_org and forbidden_org in candidate_orgs:\n",
    "        candidate_orgs.remove(forbidden_org)\n",
    "    intersection = [org for org in candidate_orgs if org in history_orgs]\n",
    "    return intersection[0] if len(intersection) == 1 else np.nan\n",
    "\n",
    "df_rest['likely_organization'] = df_rest.apply(lambda row: find_likely_org_with_rule(row, author_history, pub_orgs, forbidden_orgs_map), axis=1)\n",
    "\n",
    "rest_pass1_success = df_rest.dropna(subset=['likely_organization']).copy()\n",
    "rest_pass1_success['organization_cleaned'] = rest_pass1_success['likely_organization']\n",
    "rest_pass1_success['match_method'] = 'Rule-Based'\n",
    "rest_still_unmatched = df_rest[df_rest['likely_organization'].isna()].copy()\n",
    "print(f\"PASS 1 결과: {len(rest_pass1_success)} 건 매칭 성공.\")\n",
    "\n",
    "# =================================================================\n",
    "# Step 3.  Inference-based Matching (Pass 2)\n",
    "# =================================================================\n",
    "print(\"\\n[Step 3] PASS 2: Inference-based Matching...\")\n",
    "\n",
    "inst_location_map = df_matched.groupby('organization_cleaned')[['city', 'country']].first().to_dict('index')\n",
    "pub_org_map_inference = df_matched.groupby('pubid')['organization_cleaned'].unique().apply(list).to_dict()\n",
    "\n",
    "candidate_orgs_list = []\n",
    "for _, row in rest_still_unmatched.iterrows():\n",
    "    candidates = pub_org_map_inference.get(row['pubid'], [])\n",
    "    possible_orgs = inst[inst['pubid'] == row['pubid']]['organization_cleaned'].unique()\n",
    "    for org in possible_orgs:\n",
    "        loc = inst_location_map.get(org)\n",
    "        if loc and loc['city'] == row['city'] and loc['country'] == row['country']:\n",
    "            candidates.append(org)\n",
    "    candidate_orgs_list.append(candidates)\n",
    "rest_still_unmatched['candidate_orgs'] = candidate_orgs_list\n",
    "\n",
    "author_profiles = {}\n",
    "for _, row in pd.concat([df_matched, rest_pass1_success]).iterrows():\n",
    "    author_profiles.setdefault(row['full_name'], []).append(row['organization_cleaned'])\n",
    "for _, row in rest_still_unmatched.iterrows():\n",
    "    author_profiles.setdefault(row['full_name'], []).extend(row['candidate_orgs'])\n",
    "\n",
    "inferred_org_map = {}\n",
    "frequent_authors = set(df['full_name'].value_counts().loc[lambda x: x >= 5].index)\n",
    "for author, org_list in author_profiles.items():\n",
    "    if not org_list: continue\n",
    "    counts = Counter(org_list)\n",
    "    most_common_org, top_count = counts.most_common(1)[0]\n",
    "    if author in frequent_authors:\n",
    "        if (top_count / len(org_list)) >= 0.8:\n",
    "            inferred_org_map[author] = most_common_org\n",
    "    else:\n",
    "        inferred_org_map[author] = most_common_org\n",
    "\n",
    "rest_still_unmatched['inferred_org'] = rest_still_unmatched['full_name'].map(inferred_org_map)\n",
    "rest_pass2_success = rest_still_unmatched.dropna(subset=['inferred_org']).copy()\n",
    "rest_pass2_success['organization_cleaned'] = rest_pass2_success['inferred_org']\n",
    "rest_pass2_success['match_method'] = 'Inference-Based'\n",
    "rest_final_unmatched = rest_still_unmatched[rest_still_unmatched['inferred_org'].isna()]\n",
    "print(f\"PASS 2 Reults: {len(rest_pass2_success)} matching completed.\")\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# Step 4. Unify Final Results and Canonical Author/Institution \n",
    "# =================================================================\n",
    "print(\"\\n[Step 4] Unify final results and canonical author/institution...\")\n",
    "\n",
    "# 4-1. Integrate all data set\n",
    "df_matched = pd.merge(df_matched, df_for_id_unification[['pubid', 'author_id_te', 'author_id_te_cleaned']].drop_duplicates(), on=['pubid', 'author_id_te'], how='left')\n",
    "df_matched['match_method'] = 'Original'\n",
    "rest_final_unmatched['match_method'] = 'Unmatched'\n",
    "\n",
    "# 4-2. Combine all records \n",
    "final_output_columns = [\n",
    "    'pubid', 'city', 'country', 'pubyear', 'author_id_te',\n",
    "    'full_name', 'matched', 'organization_cleaned',\n",
    "    'author_id_te_cleaned', 'match_method'\n",
    "]\n",
    "detailed_df = pd.concat([\n",
    "    df_matched.reindex(columns=final_output_columns),\n",
    "    rest_pass1_success.reindex(columns=final_output_columns),\n",
    "    rest_pass2_success.reindex(columns=final_output_columns),\n",
    "    rest_final_unmatched.reindex(columns=final_output_columns)\n",
    "], ignore_index=True)\n",
    "\n",
    "# 4-3. Canonical institution assignment per author\n",
    "# Rule 1: Create mapping for authors with 'matched' records \n",
    "matched_records = detailed_df[detailed_df['matched'] == 'matched'].copy()\n",
    "matched_records_sorted = matched_records.sort_values(by=['author_id_te_cleaned', 'pubyear'], ascending=[True, False])\n",
    "matched_map = matched_records_sorted.drop_duplicates(subset='author_id_te_cleaned', keep='first')\n",
    "matched_map = matched_map.set_index('author_id_te_cleaned')['organization_cleaned']\n",
    "\n",
    "# Rule 2: For authors with only 'rest' records, assign the most frequent institution\n",
    "rest_map = {}\n",
    "for author_name, org_list in author_profiles.items():\n",
    "    if org_list:\n",
    "        most_common_org = Counter(org_list).most_common(1)[0][0]\n",
    "        rest_map[author_name] = most_common_org\n",
    "\n",
    "author_id_mapping = df_for_id_unification[['full_name', 'author_id_te_cleaned']].dropna().drop_duplicates()\n",
    "name_to_id = author_id_mapping.set_index('full_name')['author_id_te_cleaned']\n",
    "rest_map_by_id = {}\n",
    "for name, org in rest_map.items():\n",
    "    if name in name_to_id.index:\n",
    "        author_id = name_to_id[name]\n",
    "        rest_map_by_id[author_id] = org\n",
    "\n",
    "final_canonical_map = matched_map.combine_first(pd.Series(rest_map_by_id))\n",
    "detailed_df['organization_cleaned'] = detailed_df['author_id_te_cleaned'].map(final_canonical_map)\n",
    "\n",
    "# 4-4. Final deduplication \n",
    "final_df = detailed_df.drop_duplicates(subset=['author_id_te_cleaned', 'pubid'])\n",
    "\n",
    "# =================================================================\n",
    "# Step 5. Unify Canonical Country and City Information \n",
    "# =================================================================\n",
    "print(\"\\nStep 5. Unify Canonical Country and City Information\")\n",
    "\n",
    "# Remove records with missing organization information\n",
    "canonical_country_map = final_df.groupby('organization_cleaned')['country'].apply(lambda x: x.mode()[0] if not x.empty else None)\n",
    "final_df['country'] = final_df['organization_cleaned'].map(canonical_country_map)\n",
    "\n",
    "canonical_city_map = final_df.groupby('organization_cleaned')['city'].apply(lambda x: x.mode()[0] if not x.empty else None)\n",
    "final_df['city'] = final_df['organization_cleaned'].map(canonical_city_map)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# Step 6. Final Data Saving\n",
    "# =================================================================\n",
    "print(\"\\nStep 6. Fianal Data Saving...\")\n",
    "final_df.to_parquet(dir + 'quant_author_ed_eu_val_final_unified.parquet', index=False)\n",
    "final_df.to_csv(dir + 'quant_author_ed_eu_val_final_unified.csv', index=False)\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "total_rest = len(df_rest)\n",
    "p1_count = len(rest_pass1_success)\n",
    "p2_count = len(rest_pass2_success)\n",
    "final_fail_count = len(rest_final_unmatched)\n",
    "print(f\"Among {total_rest} 'rest' records,\")\n",
    "print(f\"  - rule-based matching successful: {p1_count} 건\")\n",
    "print(f\"  - inference-based matching successful: {p2_count} 건\")\n",
    "print(f\"  - final matching fail: {final_fail_count} 건\")\n",
    "print(f\"\\nAll completed. Final data is now at 'quant_author_ed_eu_val_final_unified.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
